{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e30356",
   "metadata": {},
   "source": [
    "# Hyperparam tuning\n",
    "\n",
    "Code [here](https://www.kaggle.com/code/bigironsphere/parameter-tuning-in-one-function-with-hyperopt/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1e1f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt import hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# #optional but advised\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab6b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a1119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some test data\n",
    "#GLOBAL HYPEROPT PARAMETERS\n",
    "NUM_EVALS = 1000 #number of hyperopt evaluation rounds\n",
    "N_FOLDS = 5 #number of cross-validation folds on data in each evaluation round\n",
    "\n",
    "#LIGHTGBM PARAMETERS\n",
    "LGBM_MAX_LEAVES = 2**8 #maximum number of leaves per tree for LightGBM\n",
    "LGBM_MAX_DEPTH = 25 #maximum tree depth for LightGBM\n",
    "EVAL_METRIC_LGBM_REG = 'mae' #LightGBM regression metric. Note that 'rmse' is more commonly used \n",
    "EVAL_METRIC_LGBM_CLASS = 'auc'#LightGBM classification metric\n",
    "\n",
    "#XGBOOST PARAMETERS\n",
    "XGB_MAX_LEAVES = 2**10 #maximum number of leaves when using histogram splitting\n",
    "XGB_MAX_DEPTH = 25 #maximum tree depth for XGBoost\n",
    "EVAL_METRIC_XGB_REG = 'mae' #XGBoost regression metric\n",
    "EVAL_METRIC_XGB_CLASS = 'auc' #XGBoost classification metric\n",
    "\n",
    "#OPTIONAL OUTPUT\n",
    "BEST_SCORE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0059ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quick_hyperopt(data, labels, package='lgbm', num_evals=NUM_EVALS, diagnostic=False):\n",
    "    \n",
    "    #==========\n",
    "    #LightGBM\n",
    "    #==========\n",
    "    \n",
    "    if package=='lgbm':\n",
    "        \n",
    "        print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth',\n",
    "                         'num_leaves',\n",
    "                         'max_bin',\n",
    "                         'min_data_in_leaf',\n",
    "                         'min_data_in_bin']\n",
    "        \n",
    "\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            #cast integer params from float to int\n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "            \n",
    "            #extract nested conditional parameters\n",
    "            if space_params['boosting']['boosting'] == 'goss':\n",
    "                top_rate = space_params['boosting'].get('top_rate')\n",
    "                other_rate = space_params['boosting'].get('other_rate')\n",
    "                #0 <= top_rate + other_rate <= 1\n",
    "                top_rate = max(top_rate, 0)\n",
    "                top_rate = min(top_rate, 0.5)\n",
    "                other_rate = max(other_rate, 0)\n",
    "                other_rate = min(other_rate, 0.5)\n",
    "                space_params['top_rate'] = top_rate\n",
    "                space_params['other_rate'] = other_rate\n",
    "            \n",
    "            subsample = space_params['boosting'].get('subsample', 1.0)\n",
    "            space_params['boosting'] = space_params['boosting']['boosting']\n",
    "            space_params['subsample'] = subsample\n",
    "            space_params['verbose'] = -1\n",
    "            \n",
    "            \n",
    "            #for classification, set stratified=True and metrics=EVAL_METRIC_LGBM_CLASS\n",
    "#             cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=False,\n",
    "#                                 early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n",
    "\n",
    "            train = lgb.Dataset(data, labels)\n",
    "\n",
    "\n",
    "            #for reg, set stratified=False and metrics=EVAL_METRIC_LGBM_REG\n",
    "            cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=True,\n",
    "                                 early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_CLASS, seed=42)\n",
    "            \n",
    "            #best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            best_loss = 1 - cv_results['auc-mean'][-1]\n",
    "            #if necessary, replace 'auc-mean' with '[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "                \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = [{'boosting': 'gbdt',\n",
    "                          'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                         {'boosting': 'goss',\n",
    "                          'subsample': 1.0,\n",
    "                         'top_rate': hp.uniform('top_rate', 0, 0.5),\n",
    "                         'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n",
    "        #metric_list = ['MAE', 'RMSE'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        metric_list = ['auc'] #modify as required for other classification metrics\n",
    "        objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n",
    "        objective_list_class = ['binary', 'cross_entropy']\n",
    "        #for classification set objective_list = objective_list_class\n",
    "        objective_list = objective_list_class\n",
    "\n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n",
    "                'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n",
    "                'max_bin': hp.quniform('max_bin', 32, 64, 2),\n",
    "                'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 128, 1),\n",
    "                'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 128, 1),\n",
    "                'min_gain_to_split' : hp.quniform('min_gain_to_split', 0.1, 5, 0.01),\n",
    "                'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n",
    "                'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'metric' : hp.choice('metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n",
    "                'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01)\n",
    "            }\n",
    "        \n",
    "        #optional: activate GPU for LightGBM\n",
    "        #follow compilation steps here:\n",
    "        #https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm/\n",
    "        #then uncomment lines below:\n",
    "        #space['device'] = 'gpu'\n",
    "        #space['gpu_platform_id'] = 0,\n",
    "        #space['gpu_device_id'] =  0\n",
    "\n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "                \n",
    "        #fmin() will return the index of values chosen from the lists/arrays in 'space'\n",
    "        #to obtain actual values, index values are used to subset the original lists/arrays\n",
    "        best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n",
    "        best['metric'] = metric_list[best['metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "                \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    #==========\n",
    "    #XGBoost\n",
    "    #==========\n",
    "    \n",
    "    if package=='xgb':\n",
    "        \n",
    "        print('Running {} rounds of XGBoost parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth']\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "                \n",
    "            #extract multiple nested tree_method conditional parameters\n",
    "            #libera te tutemet ex inferis\n",
    "            if space_params['tree_method']['tree_method'] == 'hist':\n",
    "                max_bin = space_params['tree_method'].get('max_bin')\n",
    "                space_params['max_bin'] = int(max_bin)\n",
    "                if space_params['tree_method']['grow_policy']['grow_policy']['grow_policy'] == 'depthwise':\n",
    "                    grow_policy = space_params['tree_method'].get('grow_policy').get('grow_policy').get('grow_policy')\n",
    "                    space_params['grow_policy'] = grow_policy\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "                else:\n",
    "                    max_leaves = space_params['tree_method']['grow_policy']['grow_policy'].get('max_leaves')\n",
    "                    space_params['grow_policy'] = 'lossguide'\n",
    "                    space_params['max_leaves'] = int(max_leaves)\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "            else:\n",
    "                space_params['tree_method'] = space_params['tree_method'].get('tree_method')\n",
    "                \n",
    "            #for classification replace EVAL_METRIC_XGB_REG with EVAL_METRIC_XGB_CLASS\n",
    "            cv_results = xgb.cv(space_params, train, nfold=N_FOLDS, metrics=[EVAL_METRIC_XGB_CLASS],\n",
    "                             early_stopping_rounds=100, stratified=False, seed=42)\n",
    "            \n",
    "            #best_loss = cv_results['test-mae-mean'].iloc[-1] #or 'test-rmse-mean' if using RMSE\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            best_loss = 1 - cv_results['test-auc-mean'].iloc[-1]\n",
    "            #if necessary, replace 'test-auc-mean' with 'test-[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "        train = xgb.DMatrix(data, labels)\n",
    "        \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = ['gbtree', 'gblinear'] #if including 'dart', make sure to set 'n_estimators'\n",
    "        #metric_list = ['MAE', 'RMSE'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        metric_list = ['auc']\n",
    "        #modify as required for other classification metrics classification\n",
    "        \n",
    "        tree_method = [#{'tree_method' : 'exact'},\n",
    "               #{'tree_method' : 'approx'},\n",
    "               {'tree_method' : 'hist',\n",
    "                'max_bin': hp.quniform('max_bin', 2**3, 2**7, 1),\n",
    "                'grow_policy' : {'grow_policy': {'grow_policy':'depthwise'},\n",
    "                                'grow_policy' : {'grow_policy':'lossguide',\n",
    "                                                  'max_leaves': hp.quniform('max_leaves', 32, XGB_MAX_LEAVES, 1)}}}]\n",
    "        \n",
    "        #if using GPU, replace 'exact' with 'gpu_exact' and 'hist' with\n",
    "        #'gpu_hist' in the nested dictionary above\n",
    "        \n",
    "        objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie']\n",
    "        objective_list_class = ['reg:logistic', 'binary:logistic']\n",
    "        #for classification change line below to 'objective_list = objective_list_class'\n",
    "        objective_list = objective_list_class\n",
    "        \n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'tree_method' : hp.choice('tree_method', tree_method),\n",
    "                'max_depth': hp.quniform('max_depth', 2, XGB_MAX_DEPTH, 1),\n",
    "                'reg_alpha' : hp.uniform('reg_alpha', 0, 5),\n",
    "                'reg_lambda' : hp.uniform('reg_lambda', 0, 5),\n",
    "                'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n",
    "                'gamma' : hp.uniform('gamma', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'eval_metric' : hp.choice('eval_metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n",
    "                'colsample_bynode' : hp.quniform('colsample_bynode', 0.1, 1, 0.01),\n",
    "                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),\n",
    "                'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "                'nthread' : -1\n",
    "            }\n",
    "        \n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "        \n",
    "        best['tree_method'] = tree_method[best['tree_method']]['tree_method']\n",
    "        best['boosting'] = boosting_list[best['boosting']]\n",
    "        best['eval_metric'] = metric_list[best['eval_metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "        \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        if 'max_leaves' in best:\n",
    "            best['max_leaves'] = int(best['max_leaves'])\n",
    "        if 'max_bin' in best:\n",
    "            best['max_bin'] = int(best['max_bin'])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        \n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    else:\n",
    "        print('Package not recognised. Please use \"lgbm\" for LightGBM, \"xgb\" for XGBoost or \"cb\" for CatBoost.')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075f18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart_2020_cleaned.csv')\n",
    "\n",
    "# reformat\n",
    "df['HeartDisease'] = [1 if i == 'Yes' else 0 for i in df.HeartDisease]\n",
    "\n",
    "categoricals = list(df.columns[df.dtypes == 'object'])\n",
    "\n",
    "df_cat_num = pd.get_dummies(df[categoricals],drop_first=True)\n",
    "\n",
    "# fix names\n",
    "df_cat_num = df_cat_num.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "df_num_only = df.drop(columns = categoricals)\n",
    "\n",
    "df_all = pd.concat([df_num_only, df_cat_num], axis=1)\n",
    "\n",
    "data = df_all.iloc[:,1:]\n",
    "labels = df_all['HeartDisease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13afae0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train lgb model\n",
    "lgbm_params = quick_hyperopt(X_train, y_train, 'lgbm', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d71abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 400 rounds of XGBoost parameter optimisation:\n",
      "100%|██████████| 400/400 [34:07<00:00,  5.12s/trial, best loss: 0.1638643999999999] \n",
      "{boosting: gblinear\n",
      "colsample_bylevel: 0.72\n",
      "colsample_bynode: 0.47000000000000003\n",
      "colsample_bytree: 0.92\n",
      "eval_metric: auc\n",
      "gamma: 4.270674828918584\n",
      "learning_rate: 0.19540363900814706\n",
      "max_bin: 86\n",
      "max_depth: 17\n",
      "max_leaves: 245\n",
      "min_child_weight: 2.1891587625492277\n",
      "objective: reg:logistic\n",
      "reg_alpha: 0.20145542455429888\n",
      "reg_lambda: 2.117907617599555\n",
      "subsample: 1.0\n",
      "tree_method: hist}\n"
     ]
    }
   ],
   "source": [
    "# train xgb model\n",
    "\n",
    "xgb.set_config(verbosity=0)\n",
    "xgb_params = quick_hyperopt(X_train, y_train, 'xgb', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd4e611",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lgbm_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-662eeb41b0cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlgbm_params_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlgbm_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verbose'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lgbm_params' is not defined"
     ]
    }
   ],
   "source": [
    "# try an oos evaluation\n",
    "\n",
    "lgbm_params_default={}\n",
    "\n",
    "lgbm_params['verbose'] = -1\n",
    "\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_model = lgb.train(params=lgbm_params, train_set=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36031521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 405\n",
      "[LightGBM] [Info] Number of data points in the train set: 239846, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.086051\n"
     ]
    }
   ],
   "source": [
    "# default list\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "lgb_model_default = lgb.train(params=lgbm_params_default, train_set=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f59596a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lgb_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-be768515f4a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lgb_model' is not defined"
     ]
    }
   ],
   "source": [
    "y_test_predict = lgb_model.predict(X_test)\n",
    "roc_auc_score(y_true=y_test, y_score=y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169815bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8404841555118137"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict = lgb_model_default.predict(X_test)\n",
    "roc_auc_score(y_true=y_test, y_score=y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test_predict, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29b5db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb eval with nice params\n",
    "#xgb_params['objective'] = 'binary:logistic'\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "xgb_model = xgb.train(xgb_params, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39b8a466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8314291482699596"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict_xgb = xgb_model.predict(dtest)\n",
    "roc_auc_score(y_true=y_test, y_score=y_test_predict_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48204ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8330652024874521"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use default params\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "xgb_params_default = {}\n",
    "xgb_params_default['objective'] = 'binary:logistic'\n",
    "\n",
    "xgb_model = xgb.train(xgb_params_default, dtrain)\n",
    "\n",
    "y_test_predict_xgb = xgb_model.predict(dtest)\n",
    "\n",
    "roc_auc_score(y_true=y_test, y_score=y_test_predict_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97102242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
